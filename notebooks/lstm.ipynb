{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/IMDB.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=[\"sentiment\"])\n",
    "data = data.rename({\"review\": \"sentence\", \"sentiment_negative\": \"negative\", \"sentiment_positive\": \"positive\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"sentence\"]]\n",
    "y = data.drop([\"sentence\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = X[\"sentence\"].values\n",
    "words = \" \".join(sentences)\n",
    "words = words.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, words: List[str]) -> None:\n",
    "        counter = Counter(words)\n",
    "        self.vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "        self.int2word = dict(enumerate(self.vocab, 1))\n",
    "        self.int2word[0] = \"<PAD>\"\n",
    "        self.word2int = {word: id for id, word in self.int2word.items()}\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def tokenize(self, sentence: str) -> List[str]:\n",
    "        return sentence.split()\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.word2int[word] for word in tokens]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids: List[int] | torch.LongTensor) -> List[str]:\n",
    "        if isinstance(ids, torch.LongTensor):\n",
    "            return [self.int2word[id.item()] for id in ids]\n",
    "        else: \n",
    "            return [self.int2word[id] for id in ids]\n",
    "    \n",
    "tokenizer = Tokenizer(words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, root: str = \"../data/IMDB.csv\", train: bool = True, test_size: float = 0.33, tokenizer: Tokenizer = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        data = pd.read_csv(root)\n",
    "        data = pd.get_dummies(data, columns=[\"sentiment\"])\n",
    "        data = data.rename({\"review\": \"sentence\", \n",
    "                            \"sentiment_negative\": \"negative\", \n",
    "                            \"sentiment_positive\": \"positive\"}, axis=1)\n",
    "        \n",
    "        X = data[\"sentence\"].values\n",
    "        y = data.drop([\"sentence\"], axis=1).values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "        if train:\n",
    "            self.X, self.y = X_train, y_train\n",
    "        else:\n",
    "            self.X, self.y = X_test, y_test\n",
    "\n",
    "        self.tokenizer: Tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.X[idx]\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        ids = torch.tensor(ids, dtype=torch.long)\n",
    "        labels = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return ids, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = IMDBDataset(tokenizer=tokenizer)\n",
    "test_dataset = IMDBDataset(train=False, tokenizer=tokenizer)\n",
    "\n",
    "batch_size: int = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset)\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionalLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 emb_size: int, \n",
    "                 hidden_size: int, \n",
    "                 num_stacked_layers: int = 3, \n",
    "                 dropout: float = 0.2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, num_stacked_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "vocab_size: int = 10000 # change\n",
    "emb_size: int = 256\n",
    "hidden_size: int = 512\n",
    "num_stacked_layers: int = 2\n",
    "dropout: float = 0.25\n",
    "\n",
    "model: EmotionalLSTM = EmotionalLSTM(vocab_size, emb_size, hidden_size, num_stacked_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for sentences, labels in tqdm(train_loader):\n",
    "        sentences, labels = sentences.to(\"mps\"), labels.to(\"mps\")\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sentences)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        train_loss_history.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    for sentences, labels in test_loader:\n",
    "        sentectes, labels = sentences.to(\"mps\"), labels.to(\"mps\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(sentectes)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss_history.append(loss.item())\n",
    "\n",
    "    test_loss = loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Val_loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss_history, label=\"train_loss\")\n",
    "#plt.plot(test_loss_history, label=\"val_loss\")\n",
    "plt.xlabel(\"batch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "total_samples = 0\n",
    "\n",
    "loader = test_loader\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "for batch, (X, y) in enumerate(loader):\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X.to(\"mps\"))\n",
    "    \n",
    "    y = y.to(\"mps\")\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    \n",
    "    # Convert one-hot encoded y to class indices if needed\n",
    "    if y.dim() == 2:\n",
    "        y = torch.argmax(y, dim=1)\n",
    "    \n",
    "    # Debug prints for shapes\n",
    "    correct_predictions = (predicted == y).sum().item()\n",
    "    accuracy += correct_predictions\n",
    "    total_samples += y.size(0)\n",
    "\n",
    "accuracy / (len(loader) * loader.batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
